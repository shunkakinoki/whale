{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.basic_data import *\n",
    "from skimage.util import montage\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "import re\n",
    "import json\n",
    "import cv2\n",
    "import albumentations\n",
    "from fastai.callbacks.hooks import num_features_model\n",
    "from torch.nn import L1Loss\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = json.load(open('../data/annotations.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotations': [{'class': 'fluke',\n",
       "   'height': 372.0,\n",
       "   'type': 'rect',\n",
       "   'width': 1017.0,\n",
       "   'x': 14.0,\n",
       "   'y': 97.0},\n",
       "  {'class': 'left',\n",
       "   'type': 'point',\n",
       "   'x': 50.802273527488566,\n",
       "   'y': 98.58659021176},\n",
       "  {'class': 'notch',\n",
       "   'type': 'point',\n",
       "   'x': 516.2391276137811,\n",
       "   'y': 269.48861474128864},\n",
       "  {'class': 'right',\n",
       "   'type': 'point',\n",
       "   'x': 1013.5305065138045,\n",
       "   'y': 102.10753986218477}],\n",
       " 'class': 'image',\n",
       " 'filename': '6138dce83.jpg'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SZ = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/albu/albumentations/blob/master/notebooks/example_bboxes.ipynb\n",
    "\n",
    "# Functions to visualize bounding boxes and class labels on an image. \n",
    "# Based on https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/vis.py\n",
    "\n",
    "BOX_COLOR = (255, 0, 0)\n",
    "TEXT_COLOR = (255, 255, 255)\n",
    "\n",
    "def visualize_bbox(img, bbox, class_id, class_idx_to_name, color=BOX_COLOR, thickness=2):\n",
    "    x_min, y_min, x_max, y_max = map(int, bbox)\n",
    "#     x_min, y_min, w, h = bbox\n",
    "#     x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "    class_name = class_idx_to_name[class_id]\n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(img, class_name, (x_min, y_min - int(0.3 * text_height)), cv2.FONT_HERSHEY_SIMPLEX, 0.35,TEXT_COLOR, lineType=cv2.LINE_AA)\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize(annotations, category_id_to_name):\n",
    "    img = annotations['image'].copy()\n",
    "    for idx, bbox in enumerate(annotations['bboxes']):\n",
    "        img = visualize_bbox(img, bbox, annotations['category_id'][idx], category_id_to_name)\n",
    "#     plt.figure(figsize=(12, 12))\n",
    "#     plt.imshow(img)\n",
    "    return img\n",
    "\n",
    "def get_aug(aug, min_area=0., min_visibility=0.):\n",
    "    return albumentations.Compose(aug, bbox_params={'format': 'pascal_voc', 'min_area': min_area, 'min_visibility': min_visibility, 'label_fields': ['category_id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def j2anno(j):\n",
    "    # bbox coordinates are returned in pascal voc format [x_min, y_min, x_max, y_max]\n",
    "    im = cv2.imread(f\"../data/train-{SZ}/{j['filename']}\", cv2.IMREAD_COLOR)\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    im_height, im_width, _ = im.shape\n",
    "                    \n",
    "    orig_im = cv2.imread(f\"data/train/{j['filename']}\", cv2.IMREAD_COLOR)\n",
    "    orig_im_height, orig_im_width, _ = orig_im.shape\n",
    "\n",
    "    bbox_info = [anno for anno in j['annotations'] if anno['class'] == 'fluke'][0]\n",
    "    orig_bbox = [np.clip(bbox_info['x'], 0, orig_im_width), np.clip(bbox_info['y'], 0, orig_im_height), np.clip(bbox_info['x']+bbox_info['width'], 0, orig_im_width), np.clip(bbox_info['y']+bbox_info['height'], 0, orig_im_height)]\n",
    "    bbox = [orig_bbox[0] * SZ / orig_im_width, orig_bbox[1] * SZ / orig_im_height, orig_bbox[2] * SZ / orig_im_width,  orig_bbox[3] * SZ / orig_im_height]\n",
    "    return {'image': im, 'bboxes': [bbox], 'category_id': [0]}\n",
    "                         \n",
    "cat2name = {0: 'fluke'} # unfortunately this is required by albumentations, we cannot just have bounding box coordinates on their own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = np.stack([visualize(j2anno(j[i]), cat2name) for i in range(9)])\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.axis('off')\n",
    "plt.imshow(montage(np.stack(ims), multichannel=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
